{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥-러닝 과정 MLP | 2020.02.27. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네번째 실습. Keras 모델 생성/학습 - 보스턴 집값 예측 모델\n",
    "\n",
    "![](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/06/Regression-Tutorial-with-Keras-Deep-Learning-Library-in-Python.jpg)\n",
    "\n",
    "* CRIM: per capita crime rate by town  \n",
    "* ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "* INDUS: proportion of non-retail business acres per town.\n",
    "* CHAS: Charles River dummy variable (1 if tract bounds river; 0 otherwise)\n",
    "* NOX: nitric oxides concentration (parts per 10 million)\n",
    "* RM: average number of rooms per dwelling\n",
    "* AGE: proportion of owner-occupied units built prior to 1940\n",
    "* DIS: weighted distances to five Boston employment centres\n",
    "* RAD: index of accessibility to radial highways\n",
    "* TAX: full-value property-tax rate per \\$10,000\n",
    "* PTRATIO: pupil-teacher ratio by town\n",
    "* B: 1000(Bk — 0.63)² where Bk is the proportion of blacks by town\n",
    "* LSTAT: % lower status of the population\n",
    "* MEDV: Median value of owner-occupied homes in $1000’s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.23247   0.        8.14      0.        0.538     6.142    91.7\n",
      "   3.9769    4.      307.       21.      396.9      18.72   ]\n",
      "15.2\n"
     ]
    }
   ],
   "source": [
    "# 1. 데이터 불러오기\n",
    "from keras.datasets import boston_housing\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = boston_housing.load_data()\n",
    "\n",
    "print(X_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.27224633 -0.48361547 -0.43576161 -0.25683275 -0.1652266  -0.1764426\n",
      "  0.81306188  0.1166983  -0.62624905 -0.59517003  1.14850044  0.44807713\n",
      "  0.8252202 ]\n"
     ]
    }
   ],
   "source": [
    "# 2. Data 정규화\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(X_train_scaled[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404, 13)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 32)                448       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 4,673\n",
      "Trainable params: 4,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 3. MLP 모델 생성\n",
    "model = Sequential()\n",
    "#model.add(Dense(32, input_shape(X_train.shape[1]), activation='relu'))\n",
    "model.add(Dense(32, input_dim = 13, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "#model.add(Dropout(0.3))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1))\n",
    "#model.add(Dense(1, activation='linear'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Compile - Optimizer, Loss function 설정\n",
    "model.compile(loss = 'MSE', optimizer='adam', metrics=['MSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:6: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 323 samples, validate on 81 samples\n",
      "Epoch 1/500\n",
      "323/323 [==============================] - 0s 1ms/step - loss: 552.5955 - MSE: 552.5955 - val_loss: 590.4329 - val_MSE: 590.4329\n",
      "Epoch 2/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 480.8424 - MSE: 480.8423 - val_loss: 495.9019 - val_MSE: 495.9019\n",
      "Epoch 3/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 353.8619 - MSE: 353.8619 - val_loss: 319.8737 - val_MSE: 319.8737\n",
      "Epoch 4/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 183.8578 - MSE: 183.8578 - val_loss: 144.1526 - val_MSE: 144.1526\n",
      "Epoch 5/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 86.3208 - MSE: 86.3207 - val_loss: 97.5671 - val_MSE: 97.5671\n",
      "Epoch 6/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 64.5405 - MSE: 64.5405 - val_loss: 73.9546 - val_MSE: 73.9546\n",
      "Epoch 7/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 47.9030 - MSE: 47.9030 - val_loss: 55.5104 - val_MSE: 55.5104\n",
      "Epoch 8/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 37.9341 - MSE: 37.9341 - val_loss: 41.7623 - val_MSE: 41.7623\n",
      "Epoch 9/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 32.0514 - MSE: 32.0514 - val_loss: 35.7251 - val_MSE: 35.7251\n",
      "Epoch 10/500\n",
      "323/323 [==============================] - 0s 166us/step - loss: 28.2508 - MSE: 28.2508 - val_loss: 31.2424 - val_MSE: 31.2424\n",
      "Epoch 11/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 24.8643 - MSE: 24.8643 - val_loss: 27.8191 - val_MSE: 27.8191\n",
      "Epoch 12/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 22.7656 - MSE: 22.7656 - val_loss: 24.7698 - val_MSE: 24.7698\n",
      "Epoch 13/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 20.9131 - MSE: 20.9131 - val_loss: 22.9226 - val_MSE: 22.9226\n",
      "Epoch 14/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 19.6424 - MSE: 19.6424 - val_loss: 22.2339 - val_MSE: 22.2339\n",
      "Epoch 15/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 18.1398 - MSE: 18.1398 - val_loss: 20.4370 - val_MSE: 20.4370\n",
      "Epoch 16/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 16.9307 - MSE: 16.9307 - val_loss: 19.3545 - val_MSE: 19.3545\n",
      "Epoch 17/500\n",
      "323/323 [==============================] - 0s 170us/step - loss: 15.7900 - MSE: 15.7900 - val_loss: 18.6327 - val_MSE: 18.6327\n",
      "Epoch 18/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 14.6995 - MSE: 14.6995 - val_loss: 17.3986 - val_MSE: 17.3986\n",
      "Epoch 19/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 13.8858 - MSE: 13.8858 - val_loss: 16.5494 - val_MSE: 16.5494\n",
      "Epoch 20/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 13.3131 - MSE: 13.3131 - val_loss: 16.2424 - val_MSE: 16.2424\n",
      "Epoch 21/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 12.9952 - MSE: 12.9952 - val_loss: 15.7359 - val_MSE: 15.7359\n",
      "Epoch 22/500\n",
      "323/323 [==============================] - 0s 164us/step - loss: 12.4071 - MSE: 12.4071 - val_loss: 15.2462 - val_MSE: 15.2462\n",
      "Epoch 23/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 12.0175 - MSE: 12.0175 - val_loss: 14.8581 - val_MSE: 14.8581\n",
      "Epoch 24/500\n",
      "323/323 [==============================] - 0s 167us/step - loss: 11.6464 - MSE: 11.6464 - val_loss: 14.3619 - val_MSE: 14.3619\n",
      "Epoch 25/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 11.1929 - MSE: 11.1929 - val_loss: 14.5708 - val_MSE: 14.5708\n",
      "Epoch 26/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 10.6158 - MSE: 10.6158 - val_loss: 14.3624 - val_MSE: 14.3624\n",
      "Epoch 27/500\n",
      "323/323 [==============================] - 0s 158us/step - loss: 10.5666 - MSE: 10.5666 - val_loss: 14.0471 - val_MSE: 14.0471\n",
      "Epoch 28/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 10.2077 - MSE: 10.2077 - val_loss: 13.2896 - val_MSE: 13.2896\n",
      "Epoch 29/500\n",
      "323/323 [==============================] - 0s 166us/step - loss: 10.0909 - MSE: 10.0909 - val_loss: 13.6870 - val_MSE: 13.6870\n",
      "Epoch 30/500\n",
      "323/323 [==============================] - 0s 169us/step - loss: 9.8628 - MSE: 9.8628 - val_loss: 13.5204 - val_MSE: 13.5204\n",
      "Epoch 31/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 9.6508 - MSE: 9.6508 - val_loss: 13.5601 - val_MSE: 13.5601\n",
      "Epoch 32/500\n",
      "323/323 [==============================] - 0s 158us/step - loss: 9.1536 - MSE: 9.1536 - val_loss: 13.1002 - val_MSE: 13.1002\n",
      "Epoch 33/500\n",
      "323/323 [==============================] - 0s 164us/step - loss: 9.1445 - MSE: 9.1445 - val_loss: 13.2836 - val_MSE: 13.2836\n",
      "Epoch 34/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 9.0086 - MSE: 9.0086 - val_loss: 13.1171 - val_MSE: 13.1171\n",
      "Epoch 35/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 8.8844 - MSE: 8.8844 - val_loss: 13.3065 - val_MSE: 13.3065\n",
      "Epoch 36/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 8.6790 - MSE: 8.6790 - val_loss: 13.5014 - val_MSE: 13.5014\n",
      "Epoch 37/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 8.6643 - MSE: 8.6643 - val_loss: 13.3047 - val_MSE: 13.3047\n",
      "Epoch 38/500\n",
      "323/323 [==============================] - 0s 172us/step - loss: 8.5135 - MSE: 8.5135 - val_loss: 13.2006 - val_MSE: 13.2006\n",
      "Epoch 39/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 8.4127 - MSE: 8.4127 - val_loss: 13.3638 - val_MSE: 13.3638\n",
      "Epoch 40/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 8.3358 - MSE: 8.3358 - val_loss: 12.8734 - val_MSE: 12.8734\n",
      "Epoch 41/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 8.0800 - MSE: 8.0800 - val_loss: 13.2318 - val_MSE: 13.2318\n",
      "Epoch 42/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 8.1746 - MSE: 8.1746 - val_loss: 14.1338 - val_MSE: 14.1338\n",
      "Epoch 43/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 7.8722 - MSE: 7.8722 - val_loss: 13.1568 - val_MSE: 13.1568\n",
      "Epoch 44/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 8.1128 - MSE: 8.1128 - val_loss: 12.7386 - val_MSE: 12.7386\n",
      "Epoch 45/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 7.9698 - MSE: 7.9698 - val_loss: 12.9881 - val_MSE: 12.9881\n",
      "Epoch 46/500\n",
      "323/323 [==============================] - 0s 164us/step - loss: 7.6583 - MSE: 7.6583 - val_loss: 12.9781 - val_MSE: 12.9781\n",
      "Epoch 47/500\n",
      "323/323 [==============================] - 0s 166us/step - loss: 7.8865 - MSE: 7.8865 - val_loss: 13.1056 - val_MSE: 13.1056\n",
      "Epoch 48/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 7.4092 - MSE: 7.4092 - val_loss: 12.9713 - val_MSE: 12.9713\n",
      "Epoch 49/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 7.5195 - MSE: 7.5195 - val_loss: 12.9804 - val_MSE: 12.9804\n",
      "Epoch 50/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 7.7942 - MSE: 7.7942 - val_loss: 12.2843 - val_MSE: 12.2843\n",
      "Epoch 51/500\n",
      "323/323 [==============================] - 0s 165us/step - loss: 7.5469 - MSE: 7.5469 - val_loss: 12.4796 - val_MSE: 12.4796\n",
      "Epoch 52/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 7.2097 - MSE: 7.2097 - val_loss: 13.0518 - val_MSE: 13.0518\n",
      "Epoch 53/500\n",
      "323/323 [==============================] - 0s 166us/step - loss: 7.7200 - MSE: 7.7200 - val_loss: 12.8906 - val_MSE: 12.8906\n",
      "Epoch 54/500\n",
      "323/323 [==============================] - 0s 165us/step - loss: 7.3844 - MSE: 7.3844 - val_loss: 12.4696 - val_MSE: 12.4696\n",
      "Epoch 55/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 7.0856 - MSE: 7.0856 - val_loss: 12.9038 - val_MSE: 12.9038\n",
      "Epoch 56/500\n",
      "323/323 [==============================] - 0s 164us/step - loss: 7.0101 - MSE: 7.0101 - val_loss: 12.7533 - val_MSE: 12.7533\n",
      "Epoch 57/500\n",
      "323/323 [==============================] - 0s 170us/step - loss: 6.8952 - MSE: 6.8952 - val_loss: 12.7216 - val_MSE: 12.7216\n",
      "Epoch 58/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 6.8959 - MSE: 6.8959 - val_loss: 12.4929 - val_MSE: 12.4929\n",
      "Epoch 59/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323/323 [==============================] - 0s 168us/step - loss: 6.9288 - MSE: 6.9288 - val_loss: 12.6547 - val_MSE: 12.6547\n",
      "Epoch 60/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 6.7205 - MSE: 6.7205 - val_loss: 12.7268 - val_MSE: 12.7268\n",
      "Epoch 61/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 6.7723 - MSE: 6.7723 - val_loss: 12.8931 - val_MSE: 12.8931\n",
      "Epoch 62/500\n",
      "323/323 [==============================] - 0s 199us/step - loss: 6.7228 - MSE: 6.7228 - val_loss: 12.4075 - val_MSE: 12.4076\n",
      "Epoch 63/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 7.0999 - MSE: 7.0999 - val_loss: 12.2799 - val_MSE: 12.2799\n",
      "Epoch 64/500\n",
      "323/323 [==============================] - 0s 158us/step - loss: 6.5140 - MSE: 6.5140 - val_loss: 12.4725 - val_MSE: 12.4725\n",
      "Epoch 65/500\n",
      "323/323 [==============================] - 0s 171us/step - loss: 6.4837 - MSE: 6.4837 - val_loss: 13.3095 - val_MSE: 13.3095\n",
      "Epoch 66/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 6.4523 - MSE: 6.4523 - val_loss: 12.9343 - val_MSE: 12.9343\n",
      "Epoch 67/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 6.5735 - MSE: 6.5735 - val_loss: 12.5203 - val_MSE: 12.5203\n",
      "Epoch 68/500\n",
      "323/323 [==============================] - 0s 158us/step - loss: 6.3726 - MSE: 6.3726 - val_loss: 12.1695 - val_MSE: 12.1695\n",
      "Epoch 69/500\n",
      "323/323 [==============================] - 0s 168us/step - loss: 6.5830 - MSE: 6.5830 - val_loss: 11.9817 - val_MSE: 11.9817\n",
      "Epoch 70/500\n",
      "323/323 [==============================] - 0s 166us/step - loss: 6.2830 - MSE: 6.2830 - val_loss: 11.9941 - val_MSE: 11.9941\n",
      "Epoch 71/500\n",
      "323/323 [==============================] - 0s 165us/step - loss: 6.1667 - MSE: 6.1667 - val_loss: 12.2285 - val_MSE: 12.2285\n",
      "Epoch 72/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 6.1999 - MSE: 6.1999 - val_loss: 11.6970 - val_MSE: 11.6970\n",
      "Epoch 73/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 6.0983 - MSE: 6.0983 - val_loss: 12.5697 - val_MSE: 12.5697\n",
      "Epoch 74/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 6.1891 - MSE: 6.1891 - val_loss: 12.9255 - val_MSE: 12.9255\n",
      "Epoch 75/500\n",
      "323/323 [==============================] - 0s 168us/step - loss: 6.2419 - MSE: 6.2419 - val_loss: 12.4389 - val_MSE: 12.4389\n",
      "Epoch 76/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 6.1349 - MSE: 6.1349 - val_loss: 12.4365 - val_MSE: 12.4365\n",
      "Epoch 77/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 6.0548 - MSE: 6.0548 - val_loss: 12.4638 - val_MSE: 12.4638\n",
      "Epoch 78/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 6.0195 - MSE: 6.0195 - val_loss: 12.3429 - val_MSE: 12.3429\n",
      "Epoch 79/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 5.9533 - MSE: 5.9533 - val_loss: 12.2999 - val_MSE: 12.2999\n",
      "Epoch 80/500\n",
      "323/323 [==============================] - 0s 158us/step - loss: 6.0446 - MSE: 6.0446 - val_loss: 12.2688 - val_MSE: 12.2688\n",
      "Epoch 81/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 6.0040 - MSE: 6.0040 - val_loss: 12.4520 - val_MSE: 12.4520\n",
      "Epoch 82/500\n",
      "323/323 [==============================] - 0s 164us/step - loss: 6.1900 - MSE: 6.1900 - val_loss: 12.0483 - val_MSE: 12.0483\n",
      "Epoch 83/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 5.9226 - MSE: 5.9226 - val_loss: 12.3524 - val_MSE: 12.3524\n",
      "Epoch 84/500\n",
      "323/323 [==============================] - 0s 169us/step - loss: 5.7079 - MSE: 5.7079 - val_loss: 12.0789 - val_MSE: 12.0789\n",
      "Epoch 85/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 5.5992 - MSE: 5.5992 - val_loss: 12.0573 - val_MSE: 12.0573\n",
      "Epoch 86/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 5.6073 - MSE: 5.6073 - val_loss: 12.3085 - val_MSE: 12.3085\n",
      "Epoch 87/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 5.9910 - MSE: 5.9910 - val_loss: 11.6080 - val_MSE: 11.6080\n",
      "Epoch 88/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 5.6427 - MSE: 5.6427 - val_loss: 12.1481 - val_MSE: 12.1481\n",
      "Epoch 89/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 5.7361 - MSE: 5.7361 - val_loss: 12.7327 - val_MSE: 12.7327\n",
      "Epoch 90/500\n",
      "323/323 [==============================] - 0s 164us/step - loss: 5.7340 - MSE: 5.7340 - val_loss: 12.3929 - val_MSE: 12.3929\n",
      "Epoch 91/500\n",
      "323/323 [==============================] - 0s 167us/step - loss: 5.6719 - MSE: 5.6719 - val_loss: 12.1394 - val_MSE: 12.1394\n",
      "Epoch 92/500\n",
      "323/323 [==============================] - 0s 165us/step - loss: 5.6138 - MSE: 5.6138 - val_loss: 12.1725 - val_MSE: 12.1725\n",
      "Epoch 93/500\n",
      "323/323 [==============================] - 0s 164us/step - loss: 5.6051 - MSE: 5.6051 - val_loss: 11.7211 - val_MSE: 11.7211\n",
      "Epoch 94/500\n",
      "323/323 [==============================] - 0s 167us/step - loss: 5.3864 - MSE: 5.3864 - val_loss: 11.8484 - val_MSE: 11.8484\n",
      "Epoch 95/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 5.3985 - MSE: 5.3985 - val_loss: 11.6608 - val_MSE: 11.6608\n",
      "Epoch 96/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 5.6299 - MSE: 5.6299 - val_loss: 11.9101 - val_MSE: 11.9101\n",
      "Epoch 97/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 5.4237 - MSE: 5.4237 - val_loss: 12.4446 - val_MSE: 12.4446\n",
      "Epoch 98/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 5.2721 - MSE: 5.2721 - val_loss: 11.8512 - val_MSE: 11.8512\n",
      "Epoch 99/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 5.2559 - MSE: 5.2559 - val_loss: 11.7471 - val_MSE: 11.7471\n",
      "Epoch 100/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 5.0874 - MSE: 5.0874 - val_loss: 11.6771 - val_MSE: 11.6771\n",
      "Epoch 101/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 5.2393 - MSE: 5.2393 - val_loss: 11.7939 - val_MSE: 11.7939\n",
      "Epoch 102/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 5.1993 - MSE: 5.1993 - val_loss: 11.5031 - val_MSE: 11.5031\n",
      "Epoch 103/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 4.9561 - MSE: 4.9561 - val_loss: 11.6558 - val_MSE: 11.6558\n",
      "Epoch 104/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 4.9896 - MSE: 4.9896 - val_loss: 11.8913 - val_MSE: 11.8913\n",
      "Epoch 105/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 5.6638 - MSE: 5.6638 - val_loss: 11.9198 - val_MSE: 11.9198\n",
      "Epoch 106/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 5.5445 - MSE: 5.5445 - val_loss: 11.7841 - val_MSE: 11.7841\n",
      "Epoch 107/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 5.3632 - MSE: 5.3632 - val_loss: 11.4275 - val_MSE: 11.4275\n",
      "Epoch 108/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 5.0712 - MSE: 5.0712 - val_loss: 11.7700 - val_MSE: 11.7700\n",
      "Epoch 109/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 4.8669 - MSE: 4.8669 - val_loss: 11.2939 - val_MSE: 11.2939\n",
      "Epoch 110/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 4.9528 - MSE: 4.9528 - val_loss: 11.1269 - val_MSE: 11.1269\n",
      "Epoch 111/500\n",
      "323/323 [==============================] - 0s 158us/step - loss: 5.4286 - MSE: 5.4286 - val_loss: 11.5035 - val_MSE: 11.5035\n",
      "Epoch 112/500\n",
      "323/323 [==============================] - 0s 164us/step - loss: 5.0894 - MSE: 5.0894 - val_loss: 11.9845 - val_MSE: 11.9845\n",
      "Epoch 113/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 4.7271 - MSE: 4.7271 - val_loss: 11.2802 - val_MSE: 11.2802\n",
      "Epoch 114/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 4.8567 - MSE: 4.8567 - val_loss: 11.4676 - val_MSE: 11.4676\n",
      "Epoch 115/500\n",
      "323/323 [==============================] - 0s 166us/step - loss: 5.0172 - MSE: 5.0172 - val_loss: 11.3426 - val_MSE: 11.3426\n",
      "Epoch 116/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 4.6924 - MSE: 4.6924 - val_loss: 11.6112 - val_MSE: 11.6112\n",
      "Epoch 117/500\n",
      "323/323 [==============================] - 0s 167us/step - loss: 4.7596 - MSE: 4.7596 - val_loss: 11.0908 - val_MSE: 11.0908\n",
      "Epoch 118/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323/323 [==============================] - 0s 163us/step - loss: 4.5628 - MSE: 4.5628 - val_loss: 11.2029 - val_MSE: 11.2029\n",
      "Epoch 119/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 4.5810 - MSE: 4.5810 - val_loss: 11.2392 - val_MSE: 11.2392\n",
      "Epoch 120/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 4.7656 - MSE: 4.7656 - val_loss: 12.0218 - val_MSE: 12.0218\n",
      "Epoch 121/500\n",
      "323/323 [==============================] - 0s 168us/step - loss: 4.5953 - MSE: 4.5953 - val_loss: 11.1373 - val_MSE: 11.1373\n",
      "Epoch 122/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 4.5828 - MSE: 4.5828 - val_loss: 11.5356 - val_MSE: 11.5356\n",
      "Epoch 123/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 4.6492 - MSE: 4.6492 - val_loss: 10.8235 - val_MSE: 10.8235\n",
      "Epoch 124/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 4.4523 - MSE: 4.4523 - val_loss: 11.0659 - val_MSE: 11.0659\n",
      "Epoch 125/500\n",
      "323/323 [==============================] - 0s 175us/step - loss: 4.4009 - MSE: 4.4009 - val_loss: 11.4226 - val_MSE: 11.4226\n",
      "Epoch 126/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 4.7321 - MSE: 4.7321 - val_loss: 11.2259 - val_MSE: 11.2259\n",
      "Epoch 127/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 4.4352 - MSE: 4.4352 - val_loss: 11.1442 - val_MSE: 11.1442\n",
      "Epoch 128/500\n",
      "323/323 [==============================] - 0s 165us/step - loss: 4.4516 - MSE: 4.4516 - val_loss: 10.8621 - val_MSE: 10.8621\n",
      "Epoch 129/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 4.5107 - MSE: 4.5107 - val_loss: 11.1295 - val_MSE: 11.1295\n",
      "Epoch 130/500\n",
      "323/323 [==============================] - 0s 172us/step - loss: 4.3611 - MSE: 4.3611 - val_loss: 11.4132 - val_MSE: 11.4132\n",
      "Epoch 131/500\n",
      "323/323 [==============================] - 0s 165us/step - loss: 4.3492 - MSE: 4.3492 - val_loss: 11.0119 - val_MSE: 11.0119\n",
      "Epoch 132/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 4.2229 - MSE: 4.2229 - val_loss: 11.3441 - val_MSE: 11.3441\n",
      "Epoch 133/500\n",
      "323/323 [==============================] - 0s 158us/step - loss: 5.5398 - MSE: 5.5398 - val_loss: 12.3971 - val_MSE: 12.3971\n",
      "Epoch 134/500\n",
      "323/323 [==============================] - 0s 164us/step - loss: 4.6264 - MSE: 4.6264 - val_loss: 10.9678 - val_MSE: 10.9678\n",
      "Epoch 135/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 4.3092 - MSE: 4.3092 - val_loss: 10.2424 - val_MSE: 10.2424\n",
      "Epoch 136/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 4.2484 - MSE: 4.2484 - val_loss: 10.3934 - val_MSE: 10.3934\n",
      "Epoch 137/500\n",
      "323/323 [==============================] - 0s 166us/step - loss: 4.1669 - MSE: 4.1669 - val_loss: 10.5381 - val_MSE: 10.5381\n",
      "Epoch 138/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 4.1843 - MSE: 4.1843 - val_loss: 9.8806 - val_MSE: 9.8806\n",
      "Epoch 139/500\n",
      "323/323 [==============================] - 0s 165us/step - loss: 4.2243 - MSE: 4.2243 - val_loss: 10.1229 - val_MSE: 10.1229\n",
      "Epoch 140/500\n",
      "323/323 [==============================] - 0s 166us/step - loss: 3.9824 - MSE: 3.9824 - val_loss: 10.3168 - val_MSE: 10.3168\n",
      "Epoch 141/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 4.0038 - MSE: 4.0038 - val_loss: 10.1456 - val_MSE: 10.1456\n",
      "Epoch 142/500\n",
      "323/323 [==============================] - 0s 157us/step - loss: 4.4932 - MSE: 4.4932 - val_loss: 10.2035 - val_MSE: 10.2035\n",
      "Epoch 143/500\n",
      "323/323 [==============================] - 0s 170us/step - loss: 4.5264 - MSE: 4.5264 - val_loss: 10.3570 - val_MSE: 10.3570\n",
      "Epoch 144/500\n",
      "323/323 [==============================] - 0s 165us/step - loss: 3.9756 - MSE: 3.9756 - val_loss: 10.5192 - val_MSE: 10.5192\n",
      "Epoch 145/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 3.9828 - MSE: 3.9828 - val_loss: 10.1718 - val_MSE: 10.1718\n",
      "Epoch 146/500\n",
      "323/323 [==============================] - 0s 168us/step - loss: 3.9195 - MSE: 3.9195 - val_loss: 9.9879 - val_MSE: 9.9879\n",
      "Epoch 147/500\n",
      "323/323 [==============================] - 0s 158us/step - loss: 3.8702 - MSE: 3.8702 - val_loss: 10.1090 - val_MSE: 10.1090\n",
      "Epoch 148/500\n",
      "323/323 [==============================] - 0s 165us/step - loss: 4.0570 - MSE: 4.0570 - val_loss: 10.3887 - val_MSE: 10.3887\n",
      "Epoch 149/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 4.0260 - MSE: 4.0260 - val_loss: 10.2133 - val_MSE: 10.2133\n",
      "Epoch 150/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 4.4248 - MSE: 4.4248 - val_loss: 9.9249 - val_MSE: 9.9249\n",
      "Epoch 151/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 3.7733 - MSE: 3.7733 - val_loss: 9.9284 - val_MSE: 9.9284\n",
      "Epoch 152/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 3.9153 - MSE: 3.9153 - val_loss: 10.5049 - val_MSE: 10.5049\n",
      "Epoch 153/500\n",
      "323/323 [==============================] - 0s 165us/step - loss: 3.9184 - MSE: 3.9184 - val_loss: 10.4408 - val_MSE: 10.4408\n",
      "Epoch 154/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 3.8590 - MSE: 3.8590 - val_loss: 10.3929 - val_MSE: 10.3929\n",
      "Epoch 155/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 3.8022 - MSE: 3.8022 - val_loss: 9.9750 - val_MSE: 9.9750\n",
      "Epoch 156/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 3.7683 - MSE: 3.7683 - val_loss: 9.5104 - val_MSE: 9.5104\n",
      "Epoch 157/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 3.7603 - MSE: 3.7603 - val_loss: 9.5355 - val_MSE: 9.5355\n",
      "Epoch 158/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 3.8527 - MSE: 3.8527 - val_loss: 9.9406 - val_MSE: 9.9406\n",
      "Epoch 159/500\n",
      "323/323 [==============================] - 0s 164us/step - loss: 3.6597 - MSE: 3.6597 - val_loss: 9.9430 - val_MSE: 9.9430\n",
      "Epoch 160/500\n",
      "323/323 [==============================] - 0s 170us/step - loss: 3.8027 - MSE: 3.8027 - val_loss: 10.3027 - val_MSE: 10.3027\n",
      "Epoch 161/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 3.7794 - MSE: 3.7794 - val_loss: 9.6325 - val_MSE: 9.6325\n",
      "Epoch 162/500\n",
      "323/323 [==============================] - 0s 170us/step - loss: 3.9605 - MSE: 3.9605 - val_loss: 10.0572 - val_MSE: 10.0572\n",
      "Epoch 163/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 3.9590 - MSE: 3.9590 - val_loss: 9.9126 - val_MSE: 9.9126\n",
      "Epoch 164/500\n",
      "323/323 [==============================] - 0s 165us/step - loss: 3.8018 - MSE: 3.8018 - val_loss: 9.1563 - val_MSE: 9.1563\n",
      "Epoch 165/500\n",
      "323/323 [==============================] - 0s 175us/step - loss: 3.6097 - MSE: 3.6097 - val_loss: 9.8631 - val_MSE: 9.8631\n",
      "Epoch 166/500\n",
      "323/323 [==============================] - 0s 165us/step - loss: 3.6200 - MSE: 3.6200 - val_loss: 9.8617 - val_MSE: 9.8617\n",
      "Epoch 167/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 3.4911 - MSE: 3.4911 - val_loss: 10.1430 - val_MSE: 10.1430\n",
      "Epoch 168/500\n",
      "323/323 [==============================] - 0s 165us/step - loss: 3.5220 - MSE: 3.5220 - val_loss: 9.9437 - val_MSE: 9.9437\n",
      "Epoch 169/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 3.5274 - MSE: 3.5274 - val_loss: 9.3042 - val_MSE: 9.3042\n",
      "Epoch 170/500\n",
      "323/323 [==============================] - 0s 164us/step - loss: 3.4808 - MSE: 3.4808 - val_loss: 9.6587 - val_MSE: 9.6587\n",
      "Epoch 171/500\n",
      "323/323 [==============================] - 0s 167us/step - loss: 3.4675 - MSE: 3.4675 - val_loss: 9.4247 - val_MSE: 9.4247\n",
      "Epoch 172/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 3.6890 - MSE: 3.6890 - val_loss: 9.3150 - val_MSE: 9.3150\n",
      "Epoch 173/500\n",
      "323/323 [==============================] - 0s 158us/step - loss: 3.4368 - MSE: 3.4368 - val_loss: 9.7310 - val_MSE: 9.7310\n",
      "Epoch 174/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 3.3729 - MSE: 3.3729 - val_loss: 9.8565 - val_MSE: 9.8565\n",
      "Epoch 175/500\n",
      "323/323 [==============================] - 0s 158us/step - loss: 4.3685 - MSE: 4.3685 - val_loss: 9.9264 - val_MSE: 9.9264\n",
      "Epoch 176/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 3.5528 - MSE: 3.5528 - val_loss: 9.7414 - val_MSE: 9.7414\n",
      "Epoch 177/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "323/323 [==============================] - 0s 163us/step - loss: 3.3478 - MSE: 3.3478 - val_loss: 9.4531 - val_MSE: 9.4531\n",
      "Epoch 178/500\n",
      "323/323 [==============================] - 0s 165us/step - loss: 3.4106 - MSE: 3.4106 - val_loss: 8.7733 - val_MSE: 8.7733\n",
      "Epoch 179/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 3.7000 - MSE: 3.7000 - val_loss: 8.6473 - val_MSE: 8.6473\n",
      "Epoch 180/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 3.4691 - MSE: 3.4691 - val_loss: 8.8036 - val_MSE: 8.8036\n",
      "Epoch 181/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 3.4616 - MSE: 3.4616 - val_loss: 9.2291 - val_MSE: 9.2291\n",
      "Epoch 182/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 3.8081 - MSE: 3.8081 - val_loss: 8.9811 - val_MSE: 8.9811\n",
      "Epoch 183/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 3.6454 - MSE: 3.6454 - val_loss: 8.9253 - val_MSE: 8.9253\n",
      "Epoch 184/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 3.3830 - MSE: 3.3830 - val_loss: 9.0193 - val_MSE: 9.0193\n",
      "Epoch 185/500\n",
      "323/323 [==============================] - 0s 166us/step - loss: 3.0280 - MSE: 3.0280 - val_loss: 8.9832 - val_MSE: 8.9832\n",
      "Epoch 186/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 3.3129 - MSE: 3.3129 - val_loss: 9.7054 - val_MSE: 9.7053\n",
      "Epoch 187/500\n",
      "323/323 [==============================] - 0s 174us/step - loss: 3.2004 - MSE: 3.2004 - val_loss: 9.7519 - val_MSE: 9.7519\n",
      "Epoch 188/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 3.7757 - MSE: 3.7757 - val_loss: 9.4975 - val_MSE: 9.4975\n",
      "Epoch 189/500\n",
      "323/323 [==============================] - 0s 158us/step - loss: 3.2127 - MSE: 3.2127 - val_loss: 9.3034 - val_MSE: 9.3034\n",
      "Epoch 190/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 3.2817 - MSE: 3.2817 - val_loss: 8.4824 - val_MSE: 8.4824\n",
      "Epoch 191/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 3.2320 - MSE: 3.2320 - val_loss: 8.7574 - val_MSE: 8.7574\n",
      "Epoch 192/500\n",
      "323/323 [==============================] - 0s 158us/step - loss: 3.0810 - MSE: 3.0810 - val_loss: 9.0359 - val_MSE: 9.0359\n",
      "Epoch 193/500\n",
      "323/323 [==============================] - 0s 158us/step - loss: 3.1166 - MSE: 3.1166 - val_loss: 8.8527 - val_MSE: 8.8527\n",
      "Epoch 194/500\n",
      "323/323 [==============================] - 0s 158us/step - loss: 2.9913 - MSE: 2.9913 - val_loss: 8.9241 - val_MSE: 8.9241\n",
      "Epoch 195/500\n",
      "323/323 [==============================] - 0s 158us/step - loss: 3.1571 - MSE: 3.1571 - val_loss: 8.9962 - val_MSE: 8.9962\n",
      "Epoch 196/500\n",
      "323/323 [==============================] - 0s 166us/step - loss: 3.0726 - MSE: 3.0726 - val_loss: 8.2997 - val_MSE: 8.2997\n",
      "Epoch 197/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 3.0314 - MSE: 3.0314 - val_loss: 9.0611 - val_MSE: 9.0611\n",
      "Epoch 198/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 3.2405 - MSE: 3.2405 - val_loss: 9.1067 - val_MSE: 9.1067\n",
      "Epoch 199/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 3.0524 - MSE: 3.0524 - val_loss: 9.2071 - val_MSE: 9.2071\n",
      "Epoch 200/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 3.1649 - MSE: 3.1649 - val_loss: 8.9190 - val_MSE: 8.9190\n",
      "Epoch 201/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 2.9333 - MSE: 2.9333 - val_loss: 8.8575 - val_MSE: 8.8575\n",
      "Epoch 202/500\n",
      "323/323 [==============================] - 0s 167us/step - loss: 2.8062 - MSE: 2.8062 - val_loss: 8.2941 - val_MSE: 8.2941\n",
      "Epoch 203/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 2.7990 - MSE: 2.7990 - val_loss: 7.9716 - val_MSE: 7.9716\n",
      "Epoch 204/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 2.8000 - MSE: 2.8000 - val_loss: 8.5533 - val_MSE: 8.5533\n",
      "Epoch 205/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 2.7868 - MSE: 2.7868 - val_loss: 8.2227 - val_MSE: 8.2227\n",
      "Epoch 206/500\n",
      "323/323 [==============================] - 0s 165us/step - loss: 2.9792 - MSE: 2.9792 - val_loss: 8.7806 - val_MSE: 8.7806\n",
      "Epoch 207/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 2.7879 - MSE: 2.7879 - val_loss: 8.4483 - val_MSE: 8.4483\n",
      "Epoch 208/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 2.8998 - MSE: 2.8998 - val_loss: 8.4223 - val_MSE: 8.4223\n",
      "Epoch 209/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 2.9121 - MSE: 2.9121 - val_loss: 8.4179 - val_MSE: 8.4179\n",
      "Epoch 210/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 2.6116 - MSE: 2.6116 - val_loss: 8.1714 - val_MSE: 8.1714\n",
      "Epoch 211/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 2.6589 - MSE: 2.6589 - val_loss: 7.9319 - val_MSE: 7.9319\n",
      "Epoch 212/500\n",
      "323/323 [==============================] - 0s 164us/step - loss: 2.6683 - MSE: 2.6683 - val_loss: 8.0602 - val_MSE: 8.0602\n",
      "Epoch 213/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 2.7381 - MSE: 2.7381 - val_loss: 7.9652 - val_MSE: 7.9652\n",
      "Epoch 214/500\n",
      "323/323 [==============================] - 0s 164us/step - loss: 2.6744 - MSE: 2.6744 - val_loss: 8.6298 - val_MSE: 8.6298\n",
      "Epoch 215/500\n",
      "323/323 [==============================] - 0s 166us/step - loss: 2.7800 - MSE: 2.7800 - val_loss: 7.8046 - val_MSE: 7.8046\n",
      "Epoch 216/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 2.6129 - MSE: 2.6129 - val_loss: 8.2124 - val_MSE: 8.2124\n",
      "Epoch 217/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 2.5346 - MSE: 2.5346 - val_loss: 8.0122 - val_MSE: 8.0122\n",
      "Epoch 218/500\n",
      "323/323 [==============================] - 0s 158us/step - loss: 2.5377 - MSE: 2.5377 - val_loss: 7.6649 - val_MSE: 7.6649\n",
      "Epoch 219/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 2.7727 - MSE: 2.7727 - val_loss: 7.6336 - val_MSE: 7.6336\n",
      "Epoch 220/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 2.5768 - MSE: 2.5768 - val_loss: 8.0368 - val_MSE: 8.0368\n",
      "Epoch 221/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 2.7180 - MSE: 2.7180 - val_loss: 8.0716 - val_MSE: 8.0716\n",
      "Epoch 222/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 2.5334 - MSE: 2.5334 - val_loss: 8.4403 - val_MSE: 8.4403\n",
      "Epoch 223/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 2.9169 - MSE: 2.9169 - val_loss: 7.9670 - val_MSE: 7.9670\n",
      "Epoch 224/500\n",
      "323/323 [==============================] - 0s 166us/step - loss: 2.5134 - MSE: 2.5134 - val_loss: 7.7974 - val_MSE: 7.7974\n",
      "Epoch 225/500\n",
      "323/323 [==============================] - 0s 165us/step - loss: 2.6446 - MSE: 2.6446 - val_loss: 7.3831 - val_MSE: 7.3831\n",
      "Epoch 226/500\n",
      "323/323 [==============================] - 0s 166us/step - loss: 2.4919 - MSE: 2.4919 - val_loss: 7.9468 - val_MSE: 7.9468\n",
      "Epoch 227/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 2.7039 - MSE: 2.7039 - val_loss: 7.8305 - val_MSE: 7.8305\n",
      "Epoch 228/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 2.5059 - MSE: 2.5059 - val_loss: 7.4880 - val_MSE: 7.4880\n",
      "Epoch 229/500\n",
      "323/323 [==============================] - 0s 158us/step - loss: 2.5682 - MSE: 2.5682 - val_loss: 7.3405 - val_MSE: 7.3405\n",
      "Epoch 230/500\n",
      "323/323 [==============================] - 0s 157us/step - loss: 2.9162 - MSE: 2.9162 - val_loss: 7.8842 - val_MSE: 7.8842\n",
      "Epoch 231/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 2.4934 - MSE: 2.4934 - val_loss: 7.4556 - val_MSE: 7.4556\n",
      "Epoch 232/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 2.3451 - MSE: 2.3451 - val_loss: 7.4073 - val_MSE: 7.4073\n",
      "Epoch 233/500\n",
      "323/323 [==============================] - 0s 164us/step - loss: 2.4622 - MSE: 2.4622 - val_loss: 7.7475 - val_MSE: 7.7475\n",
      "Epoch 234/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 2.5116 - MSE: 2.5116 - val_loss: 7.8762 - val_MSE: 7.8762\n",
      "Epoch 235/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 2.4329 - MSE: 2.4329 - val_loss: 7.3117 - val_MSE: 7.3117\n",
      "Epoch 236/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 2.3754 - MSE: 2.3754 - val_loss: 7.1501 - val_MSE: 7.1501\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 237/500\n",
      "323/323 [==============================] - 0s 158us/step - loss: 2.3345 - MSE: 2.3345 - val_loss: 7.3403 - val_MSE: 7.3403\n",
      "Epoch 238/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 2.4528 - MSE: 2.4528 - val_loss: 7.7823 - val_MSE: 7.7823\n",
      "Epoch 239/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 2.3058 - MSE: 2.3058 - val_loss: 7.1091 - val_MSE: 7.1091\n",
      "Epoch 240/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 2.4069 - MSE: 2.4069 - val_loss: 8.0712 - val_MSE: 8.0712\n",
      "Epoch 241/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 2.6329 - MSE: 2.6329 - val_loss: 7.3361 - val_MSE: 7.3361\n",
      "Epoch 242/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 2.3109 - MSE: 2.3109 - val_loss: 7.2461 - val_MSE: 7.2462\n",
      "Epoch 243/500\n",
      "323/323 [==============================] - 0s 167us/step - loss: 2.3211 - MSE: 2.3211 - val_loss: 7.0199 - val_MSE: 7.0199\n",
      "Epoch 244/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 2.3203 - MSE: 2.3203 - val_loss: 7.5572 - val_MSE: 7.5572\n",
      "Epoch 245/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 3.2430 - MSE: 3.2430 - val_loss: 7.2667 - val_MSE: 7.2667\n",
      "Epoch 246/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 2.3782 - MSE: 2.3782 - val_loss: 7.2282 - val_MSE: 7.2282\n",
      "Epoch 247/500\n",
      "323/323 [==============================] - 0s 186us/step - loss: 2.3169 - MSE: 2.3169 - val_loss: 7.1546 - val_MSE: 7.1546\n",
      "Epoch 248/500\n",
      "323/323 [==============================] - 0s 157us/step - loss: 2.2481 - MSE: 2.2481 - val_loss: 7.5916 - val_MSE: 7.5916\n",
      "Epoch 249/500\n",
      "323/323 [==============================] - 0s 158us/step - loss: 2.1380 - MSE: 2.1380 - val_loss: 7.1946 - val_MSE: 7.1946\n",
      "Epoch 250/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 2.0983 - MSE: 2.0983 - val_loss: 7.3021 - val_MSE: 7.3021\n",
      "Epoch 251/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 2.0826 - MSE: 2.0826 - val_loss: 7.4057 - val_MSE: 7.4057\n",
      "Epoch 252/500\n",
      "323/323 [==============================] - 0s 167us/step - loss: 2.1216 - MSE: 2.1216 - val_loss: 7.3857 - val_MSE: 7.3857\n",
      "Epoch 253/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 2.2314 - MSE: 2.2314 - val_loss: 7.0487 - val_MSE: 7.0487\n",
      "Epoch 254/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 2.7951 - MSE: 2.7951 - val_loss: 7.1922 - val_MSE: 7.1922\n",
      "Epoch 255/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 2.5828 - MSE: 2.5828 - val_loss: 7.8791 - val_MSE: 7.8791\n",
      "Epoch 256/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 2.4808 - MSE: 2.4808 - val_loss: 6.8756 - val_MSE: 6.8756\n",
      "Epoch 257/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 2.2223 - MSE: 2.2223 - val_loss: 7.4945 - val_MSE: 7.4945\n",
      "Epoch 258/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 2.1095 - MSE: 2.1095 - val_loss: 7.1140 - val_MSE: 7.1140\n",
      "Epoch 259/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 2.0460 - MSE: 2.0460 - val_loss: 7.0056 - val_MSE: 7.0056\n",
      "Epoch 260/500\n",
      "323/323 [==============================] - 0s 165us/step - loss: 2.1158 - MSE: 2.1158 - val_loss: 6.9473 - val_MSE: 6.9473\n",
      "Epoch 261/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 2.1521 - MSE: 2.1521 - val_loss: 6.8428 - val_MSE: 6.8428\n",
      "Epoch 262/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 1.9967 - MSE: 1.9967 - val_loss: 7.0222 - val_MSE: 7.0222\n",
      "Epoch 263/500\n",
      "323/323 [==============================] - 0s 170us/step - loss: 1.9309 - MSE: 1.9309 - val_loss: 6.8667 - val_MSE: 6.8667\n",
      "Epoch 264/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 1.9359 - MSE: 1.9359 - val_loss: 7.2325 - val_MSE: 7.2325\n",
      "Epoch 265/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 2.1276 - MSE: 2.1276 - val_loss: 7.4952 - val_MSE: 7.4952\n",
      "Epoch 266/500\n",
      "323/323 [==============================] - 0s 158us/step - loss: 2.1322 - MSE: 2.1322 - val_loss: 7.2120 - val_MSE: 7.2120\n",
      "Epoch 267/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 2.0477 - MSE: 2.0477 - val_loss: 7.6302 - val_MSE: 7.6302\n",
      "Epoch 268/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 2.1201 - MSE: 2.1201 - val_loss: 6.9378 - val_MSE: 6.9378\n",
      "Epoch 269/500\n",
      "323/323 [==============================] - 0s 165us/step - loss: 2.1744 - MSE: 2.1744 - val_loss: 6.9424 - val_MSE: 6.9424\n",
      "Epoch 270/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 1.9034 - MSE: 1.9034 - val_loss: 7.1828 - val_MSE: 7.1828\n",
      "Epoch 271/500\n",
      "323/323 [==============================] - 0s 166us/step - loss: 2.5205 - MSE: 2.5205 - val_loss: 7.1731 - val_MSE: 7.1731\n",
      "Epoch 272/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 1.9904 - MSE: 1.9904 - val_loss: 7.0975 - val_MSE: 7.0975\n",
      "Epoch 273/500\n",
      "323/323 [==============================] - 0s 162us/step - loss: 1.9168 - MSE: 1.9168 - val_loss: 7.3090 - val_MSE: 7.3090\n",
      "Epoch 274/500\n",
      "323/323 [==============================] - 0s 199us/step - loss: 1.9507 - MSE: 1.9507 - val_loss: 6.9640 - val_MSE: 6.9640\n",
      "Epoch 275/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 1.8984 - MSE: 1.8984 - val_loss: 7.1263 - val_MSE: 7.1263\n",
      "Epoch 276/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 1.9135 - MSE: 1.9135 - val_loss: 7.0550 - val_MSE: 7.0550\n",
      "Epoch 277/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 1.8208 - MSE: 1.8208 - val_loss: 6.8048 - val_MSE: 6.8048\n",
      "Epoch 278/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 1.8570 - MSE: 1.8570 - val_loss: 7.4965 - val_MSE: 7.4965\n",
      "Epoch 279/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 2.0250 - MSE: 2.0250 - val_loss: 7.4778 - val_MSE: 7.4778\n",
      "Epoch 280/500\n",
      "323/323 [==============================] - 0s 174us/step - loss: 2.3272 - MSE: 2.3272 - val_loss: 7.0560 - val_MSE: 7.0560\n",
      "Epoch 281/500\n",
      "323/323 [==============================] - 0s 165us/step - loss: 1.9808 - MSE: 1.9808 - val_loss: 7.2112 - val_MSE: 7.2112\n",
      "Epoch 282/500\n",
      "323/323 [==============================] - 0s 164us/step - loss: 2.0757 - MSE: 2.0757 - val_loss: 7.3251 - val_MSE: 7.3251\n",
      "Epoch 283/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 1.8662 - MSE: 1.8662 - val_loss: 7.0074 - val_MSE: 7.0074\n",
      "Epoch 284/500\n",
      "323/323 [==============================] - 0s 160us/step - loss: 1.7758 - MSE: 1.7758 - val_loss: 7.0038 - val_MSE: 7.0038\n",
      "Epoch 285/500\n",
      "323/323 [==============================] - 0s 164us/step - loss: 1.8196 - MSE: 1.8196 - val_loss: 7.2953 - val_MSE: 7.2953\n",
      "Epoch 286/500\n",
      "323/323 [==============================] - 0s 186us/step - loss: 1.8599 - MSE: 1.8599 - val_loss: 7.1333 - val_MSE: 7.1333\n",
      "Epoch 287/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 1.7879 - MSE: 1.7879 - val_loss: 7.0111 - val_MSE: 7.0111\n",
      "Epoch 288/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 1.8846 - MSE: 1.8846 - val_loss: 7.0907 - val_MSE: 7.0907\n",
      "Epoch 289/500\n",
      "323/323 [==============================] - 0s 166us/step - loss: 1.8755 - MSE: 1.8755 - val_loss: 7.0807 - val_MSE: 7.0807\n",
      "Epoch 290/500\n",
      "323/323 [==============================] - 0s 174us/step - loss: 1.7548 - MSE: 1.7548 - val_loss: 7.0084 - val_MSE: 7.0084\n",
      "Epoch 291/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 1.7546 - MSE: 1.7546 - val_loss: 7.4743 - val_MSE: 7.4743\n",
      "Epoch 292/500\n",
      "323/323 [==============================] - 0s 166us/step - loss: 2.4766 - MSE: 2.4766 - val_loss: 7.7606 - val_MSE: 7.7606\n",
      "Epoch 293/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 2.2114 - MSE: 2.2114 - val_loss: 7.4296 - val_MSE: 7.4296\n",
      "Epoch 294/500\n",
      "323/323 [==============================] - 0s 178us/step - loss: 1.7726 - MSE: 1.7726 - val_loss: 7.1736 - val_MSE: 7.1736\n",
      "Epoch 295/500\n",
      "323/323 [==============================] - 0s 159us/step - loss: 1.6723 - MSE: 1.6723 - val_loss: 7.2026 - val_MSE: 7.2026\n",
      "Epoch 296/500\n",
      "323/323 [==============================] - 0s 163us/step - loss: 1.7146 - MSE: 1.7146 - val_loss: 7.0399 - val_MSE: 7.0399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297/500\n",
      "323/323 [==============================] - 0s 161us/step - loss: 1.8513 - MSE: 1.8513 - val_loss: 6.8749 - val_MSE: 6.8749\n"
     ]
    }
   ],
   "source": [
    "# 5.학습시키기\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(patience=20)\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train, validation_split=0.2,  batch_size=20, nb_epoch =  500, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404/404 [==============================] - 0s 42us/step\n",
      "102/102 [==============================] - 0s 47us/step\n",
      "[2.599524800140079, 2.599524736404419]\n",
      "[18.564324173272826, 18.56432342529297]\n"
     ]
    }
   ],
   "source": [
    "# 6. Model Evaluate\n",
    "train_mse = model.evaluate(X_train_scaled, y_train)\n",
    "test_mse = model.evaluate(X_test_scaled, y_test)\n",
    "\n",
    "print(train_mse)\n",
    "print(test_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 7. 학습 시각화하기\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.plot(history.history['MSE'])\n",
    "#plt.plot(history.history['val_accuracy'])\n",
    "#plt.title(\"MSE\")\n",
    "#plt.ylabel('MSE')\n",
    "#plt.xlabel('EPOCH')\n",
    "#plt.legend(['train', 'val'], loc = 'upper left')\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc = 'upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
